{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40c67b3ff0fa04587dec508363308adaa3ceaf34",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4kjcC6QqywWl"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import json\n",
    "import pydicom\n",
    "import skimage.io\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import glob \n",
    "from skimage.transform import resize\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1c19a9314ad58a632687500c7950fbe9aad121d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../rsna-pneumonia-detection-challenge/'\n",
    "ROOT_DIR = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cb852e262b69d348743767d675573368ab672c9",
    "colab_type": "text",
    "id": "9RlMo04ckd98"
   },
   "source": [
    "### Get annotation main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "793b1c6c6ba4e5f0d51e130080aa799f230b5ef6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "EdhUEFDr0yDA",
    "outputId": "1715a5df-a577-41fd-bf20-f1a27aadb28c"
   },
   "outputs": [],
   "source": [
    "train_dicom_dir = os.path.join(DATA_DIR, 'stage_1_train_images')\n",
    "test_dicom_dir = os.path.join(DATA_DIR, 'stage_1_test_images')\n",
    "\n",
    "def parse_dataset(anns): \n",
    "    image_annotations = []\n",
    "    for index, row in anns.iterrows():\n",
    "        row['path'] = os.path.join(train_dicom_dir, row['patientId']+'.dcm')\n",
    "        image_annotations.append(row)\n",
    "    return image_annotations \n",
    "\n",
    "# training dataset\n",
    "anns = pd.read_csv(os.path.join(DATA_DIR, 'stage_1_train_labels.csv'))\n",
    "image_annotations = parse_dataset(anns=anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1d180fbd94343b00ed136aa40887d92fce3d5b0"
   },
   "source": [
    "### Get  annotation second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd7079a077030211cfa8b791041250dfe18417da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Data_Entry_2017.csv')\n",
    "data = data[~data['Finding Labels'].str.contains('Pneumonia', regex=False)]\n",
    "fnames = np.array(data['Image Index'])\n",
    "\n",
    "paths_to_files_without_pneumonia = []\n",
    "path_to_images = '../data/images_{part}/images'\n",
    "for part in ('001','002','003','004','005','006','007','008','009','010','011','012'):\n",
    "    path_to_part = path_to_images.replace('{part}', part)\n",
    "    names_in_dir = np.array(next(os.walk(path_to_part))[2])\n",
    "    paths_to_files_without_pneumonia += [os.path.join(path_to_part, fname) for fname in names_in_dir]\n",
    "    \n",
    "second_dataset = []\n",
    "for path in paths_to_files_without_pneumonia:\n",
    "    img_info = pd.Series({\n",
    "            'x':float('Nan'), 'y':float('Nan'), \n",
    "            'width':float('Nan'), 'height':float('Nan'), \n",
    "            'Target':0, \n",
    "            'path':path,\n",
    "            'patientId':path.split('/')[-1].split('.')[0]})\n",
    "    second_dataset.append(img_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9d971f496d92c4e021a17350d706180b777dbba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_dataset = image_annotations + second_dataset\n",
    "len(merge_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5da7a520f04dc548ea654a31858f891c04d4eb1"
   },
   "source": [
    "### Create dataget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5edd38c5c426638937308bafe6a023964e15708",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class train_data_generator:\n",
    "    \n",
    "    def create(dataset_info, batch_size, shape, border=128):\n",
    "        while True:\n",
    "                        \n",
    "            batch_info = [dataset_info[i] for i in np.random.choice(len(dataset_info), batch_size)]\n",
    "            batch_crop_image = np.empty((batch_size, shape[0], shape[1], shape[2]))\n",
    "            batch_labels = np.zeros((batch_size,1))\n",
    "            for i, e in enumerate(batch_info):\n",
    "                \n",
    "                #get fullsize image\n",
    "                fp = e['path']\n",
    "                full_image = None\n",
    "                if fp.split('.')[-1] == 'dcm':\n",
    "                    ds = pydicom.read_file(fp)\n",
    "                    full_image = ds.pixel_array\n",
    "                if fp.split('.')[-1] == 'jpeg' or fp.split('.')[-1] == 'png':\n",
    "                    full_image = skimage.io.imread(fp)\n",
    "                \n",
    "                # crop image\n",
    "                if not e['Target']:\n",
    "                    x = np.random.randint(border, full_image.shape[0]-shape[0]-border)\n",
    "                    y = np.random.randint(border, full_image.shape[1]-shape[1]-border)\n",
    "                else:\n",
    "                    x = int(e['x'] + (e['width']/2) - (shape[0]/2))\n",
    "                    y = int(e['y'] + (e['height']/2) - (shape[1]/2))\n",
    "                    batch_labels[i] = 1\n",
    "                    \n",
    "                crop_image = full_image[y:y+shape[0], x:x+shape[1]].copy()\n",
    "                \n",
    "                if np.random.uniform(0,1) > 0.5:\n",
    "                    crop_image = np.fliplr(crop_image)\n",
    "                \n",
    "                if crop_image.shape[:2] != (shape[0], shape[1]):\n",
    "                    break\n",
    "                \n",
    "                # If grayscale. Convert to RGB for consistency.\n",
    "                if len(crop_image.shape) == 3:\n",
    "                    if crop_image.shape[2] > 3:\n",
    "                        crop_image = crop_image[:,:,0]\n",
    "                if len(crop_image.shape) != 3 or crop_image.shape[2] != 3:\n",
    "                    crop_image = np.stack((crop_image,) * 3, -1)\n",
    "                batch_crop_image[i] = crop_image\n",
    "            yield batch_crop_image/255, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22573a80bc2c7556ee4e8fc7f1ab153602f8e88b"
   },
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "641de2a192a9112a2996812fbaf3f9a8fe74d60a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "def create_model(input_shape, n_out):\n",
    "    pretrain_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    model = Sequential()\n",
    "    model.add(pretrain_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_out))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65ba622e9b74477bfd2a4530c61cfa430bd88c12",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "if os.path.exists('../keras.model'):\n",
    "    model = load_model('../keras.model')\n",
    "elif os.path.exists('../inceptionresnetv2-classificator/keras.model'):\n",
    "    model = load_model('../inceptionresnetv2-classificator/keras.model')\n",
    "else:\n",
    "    model = create_model(\n",
    "        input_shape=(299,299,3),\n",
    "        n_out=1)\n",
    "\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=Adam(0.000001), \n",
    "    metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9eeed2015e90cb143fc231288233fe6a0df54d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50; batch_size = 16\n",
    "checkpointer = ModelCheckpoint(\n",
    "    '../keras.model', \n",
    "    verbose=2, \n",
    "    save_best_only=True)\n",
    "\n",
    "train_generator = train_data_generator.create(\n",
    "    merge_dataset, batch_size, (299,299,3))\n",
    "validation_generator = train_data_generator.create(\n",
    "    image_annotations, 100, (299,299,3))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=next(validation_generator),\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28ac95ae4e605481f32717dcf007639f73da112f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n",
    "ax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c045689c13f9d556ce972aab259925cbca94ec58",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('../keras.model'):\n",
    "    model = load_model('../keras.model')\n",
    "elif os.path.exists('../inceptionresnetv2-classificator/keras.model'):\n",
    "    model = load_model('../inceptionresnetv2-classificator/keras.model')\n",
    "    !cp \"../inceptionresnetv2-classificator/keras.model\" \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea34048c1f67ec222be16bb0e5de8ea6a52409f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_generator = train_data_generator.create(\n",
    "    image_annotations, 100, (299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04966a7b700cae01b1ba5378bb7bb8d194cd87f1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = []; predict_labels = []; \n",
    "for i in range(50):\n",
    "    imgs, labels = next(validation_generator)\n",
    "    true_labels += [int(label) for label in labels]\n",
    "    predict_labels += [float(predict) for predict in model.predict(imgs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f73cc0b7caf58fdbafea637692468293a902d5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = np.array(true_labels)\n",
    "predict_labels = (np.array(predict_labels) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd7c15a010f80ae8e4e164b5894a9124952f9ff7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(true_labels, predict_labels),\n",
    "    columns=[['predict', 'predict'],['N', 'P']],\n",
    "    index=[['true', 'true'],['N', 'P']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c1a0fa17e4d33690c67a2753024a447538f752e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(true_labels, predict_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e7812f4ddf4fdc0c66e2cb26dae59ac49592225",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lesson-3-rsna-pneumonia-detection-challenge-kaggle",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
